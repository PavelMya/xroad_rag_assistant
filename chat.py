import os
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory

# –ó–∞–≥—Ä—É–∑–∫–∞ API-–∫–ª—é—á–∞
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
embeddings = OpenAIEmbeddings(api_key=openai_api_key)
db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

# –ú–æ–¥–µ–ª—å
llm = ChatOpenAI(temperature=0, api_key=openai_api_key)

# –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AcuRAI-—Ñ–æ—Ä–º–∞—Ç–∞
prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are an expert assistant for the X-Road system, Linux system administration, and API development.\n"
     "Use the provided documentation context ({context}) as your most trusted source of truth.\n\n"
     "You follow the AcuRAI instruction format internally to understand every user question.\n"
     "Before answering, you ALWAYS interpret the user input by breaking it into these fields:\n"
     "- task: what the user is trying to achieve or fix (e.g. 'troubleshoot service startup')\n"
     "- system: what system or component is involved (e.g. 'X-Road Central Server')\n"
     "- symptom: what is going wrong or being observed (e.g. '502 Bad Gateway')\n"
     "- context: any extra conditions or constraints\n\n"
     "‚úÖ If the issue is X-Road related, use the documentation and your expertise.\n"
     "üìå If it's a general Linux/system issue, begin your reply with:\n"
     "'üìå This issue appears to be outside the scope of X-Road documentation.'\n"
     "Then cautiously suggest possible causes (no hallucination).\n"
     "‚ùå If the question is unrelated (e.g. food, weather), politely refuse to answer.\n\n"
     "‚ö†Ô∏è Be clear when unsure. Suggest checking logs or consulting system admins.\n"
     "üéØ Include example config, commands, or diagnostics where relevant.\n"
     "Return your response as a dictionary with keys: task, system, symptom, context, answer.\n"
     "Never invent services or commands that don‚Äôt exist.\n"
     "You must always reply in the same language as the user's question."),

    ("human", "Context:\n{context}\n\nQuestion:\n{question}")
])

# –ü–∞–º—è—Ç—å –∏ —Ü–µ–ø–æ—á–∫–∞
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    input_key="question",
    output_key="answer"
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=db.as_retriever(search_kwargs={"k": 4}),
    memory=memory,
    return_source_documents=True,
    output_key="answer",
    combine_docs_chain_kwargs={"prompt": prompt}  # –ø—Ä–∏–º–µ–Ω—è–µ–º AcuRAI prompt
)

import os
from dotenv import load_dotenv

from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.2,
    api_key=OPENAI_API_KEY
)

# Acurai prompt template
acurai_prompt = PromptTemplate.from_template("""
You are an expert technical assistant for X-Road documentation.
Follow the structured reasoning steps below, but output ONLY the final ANSWER to the user.

QUESTION: {question}
TASK: Determine what the user is trying to achieve.
SYMPTOM: Identify any implicit or explicit problem.
CONTEXT: Use the relevant documentation provided below:
{context}
ANSWER: Provide a clear, helpful, and technical answer.
Only return the ANSWER section in your response.
""")

# –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞
vectorstore = FAISS.load_local(
    folder_path="faiss_index",
    embeddings=OpenAIEmbeddings(api_key=OPENAI_API_KEY),
    allow_dangerous_deserialization=True
)

# –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ –±–µ–∑ –ø–∞–º—è—Ç–∏ (–∏–ª–∏ –º–æ–∂–Ω–æ –ø–æ–∑–∂–µ –∑–∞–º–µ–Ω–∏—Ç—å)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True,
    combine_docs_chain_kwargs={"prompt": acurai_prompt},
    output_key="answer",  # –£–∫–∞–∑–∞–Ω–æ —è–≤–Ω–æ
    verbose=True
)

# –§—É–Ω–∫—Ü–∏—è —á–∞—Ç–∞
def enhanced_query(query: str) -> dict:
    """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏."""
    result = qa_chain.invoke({
        "question": query,
        "chat_history": memory.chat_memory.messages  # üî• –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!
    })
    return {
        "answer": result["answer"],
        "source_documents": [
            doc.metadata.get("source", "") for doc in result["source_documents"]
        ],
        "chat_history": memory.chat_memory.messages
    }

# –¢–µ—Å—Ç –≤ –∫–æ–Ω—Å–æ–ª–∏
if __name__ == "__main__":
    while True:
        user_input = input("–í—ã: ")
        if user_input.lower() in ("–≤—ã—Ö–æ–¥", "exit", "quit"):
            break
        response = enhanced_query(user_input)
        print("GPT:", response["answer"])
import os
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.agents import Tool, initialize_agent

# üîß –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def ping_host(host: str) -> str:
    import subprocess
    try:
        output = subprocess.check_output(["ping", "-c", "1", host], stderr=subprocess.STDOUT)
        return output.decode()
    except subprocess.CalledProcessError as e:
        return f"Ping failed: {e.output.decode()}"

def read_log(file_path: str) -> str:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()[-2000:]  # –ø–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤
    except Exception as e:
        return f"Error reading log: {str(e)}"

# üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–∞
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# üîç –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞
embeddings = OpenAIEmbeddings(api_key=api_key)
db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

# ü§ñ –ú–æ–¥–µ–ª—å
llm = ChatOpenAI(temperature=0, api_key=api_key, model="gpt-4o")

# üõ†Ô∏è –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–≥–µ–Ω—Ç–∞
tools = [
    Tool.from_function(ping_host, name="ping_host", description="Ping a host. Input should be a hostname or IP."),
    Tool.from_function(read_log, name="read_log", description="Read contents of a log file.")
]

# üß† –ü–∞–º—è—Ç—å
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    input_key="question",
    output_key="answer"
)

# üßæ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (AcuRAI + Function Calling + Clarifications)
prompt = ChatPromptTemplate.from_messages([
    ("system",
    "You are a professional consultant for X-Road, Linux server infrastructure, and API operations.\n"
    "Your goal is to identify the user's intent and provide precise, technical answers based on documentation.\n\n"
    "You must extract and reason about these fields from the question:\n"
    "- task\n- system\n- symptom\n- context\n\n"
    "Then reply with a JSON object with keys: task, system, symptom, context, answer, confidence (High/Medium/Low).\n"
    "Use examples, logs, commands, or config snippets when possible.\n"
    "If question is unclear ‚Äî ask for clarification.\n"
    "If issue is outside X-Road (e.g. Linux/system errors) ‚Äî state it and offer limited help.\n"
    "Never invent answers. Always reply in the same language as the user."),
    ("human", "Context:\n{context}\n\nQuestion:\n{question}")
])

# üîó –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=db.as_retriever(search_kwargs={"k": 6}),
    memory=memory,
    return_source_documents=True,
    output_key="answer",
    combine_docs_chain_kwargs={"prompt": prompt}
)

# ü§ñ –ê–≥–µ–Ω—Ç –¥–ª—è tool-based fallback
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type="openai-functions",
    verbose=True
)

# üîÅ –û–±—ë—Ä—Ç–∫–∞ —Å —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º –∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏
def enhanced_query(question: str) -> dict:
    result = qa_chain.invoke({"question": question})

    # –£—Ç–æ—á–Ω—è—é—â–∏–π prompt
    if "please clarify" in result.get("answer", "").lower():
        result["clarify"] = True

    # –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
    if any(kw in question.lower() for kw in ["ping", "log", "restart", "uptime", "syslog"]):
        try:
            tool_output = agent.run(question)
            result["tool_output"] = tool_output
        except Exception as e:
            result["tool_output"] = f"‚ö†Ô∏è Tool error: {str(e)}"

    return result
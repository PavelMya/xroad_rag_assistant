import os
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import create_history_aware_retriever, create_retrieval_chain

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.2,
    api_key=OPENAI_API_KEY
)

# –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ FAISS
vectorstore = FAISS.load_local(
    folder_path="faiss_index",
    embeddings=OpenAIEmbeddings(api_key=OPENAI_API_KEY),
    allow_dangerous_deserialization=True
)

# –ü–∞–º—è—Ç—å —á–∞—Ç–∞
memory = ConversationBufferMemory(
    return_messages=True,
    memory_key="chat_history"
)

# --- SYSTEM PROMPT –¥–ª—è AcuRAI ---
acurai_system = SystemMessage(
    content="""
You are a senior assistant for system administrators using X-Road.

Your job is to deeply analyze the user's technical problem and give a complete, precise and practical answer.

Always reason through the following steps (internally), but show only the final ANSWER.

---
ANSWER:
‚úÖ What the user is trying to do
üîç Possible root causes (numbered list)
üîß Steps to investigate:
- With **commands**, **paths**, **logs**
- What to look for (e.g., errors, confirmation lines)
üìÑ Example config snippets (if applicable)
üìÇ Exact file locations and required permissions
üìò Source documentation references (with filenames or markdown anchors)
üß™ Can the issue be verified via CLI or UI?
ü™µ What logs to check?
‚ö†Ô∏è Are there tricky or common mistakes?

Style:
- Use markdown formatting (headers, code blocks, bullet points)
- Use emoji bullets for clarity
- Avoid vague phrases ‚Äî be concrete

Only output the ANSWER section. Do not show your reasoning process.
"""
)

# --- PROMPT –¥–ª—è AcuRAI —Ä–µ–∂–∏–º–∞ (–±–µ–∑ context!) ---
acurai_prompt = ChatPromptTemplate.from_messages([
    acurai_system,
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])

# --- PROMPT –¥–ª—è –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ ---
friendly_prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –ø—Ä–æ—Å—Ç–æ, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏ –≤–µ–∂–ª–∏–≤–æ."),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])

# --- –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏-–æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π retriever ---
retriever = create_history_aware_retriever(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    prompt=acurai_prompt
)

combine_chain = acurai_prompt | llm

acurai_chain = create_retrieval_chain(
    retriever=retriever,
    combine_docs_chain=combine_chain
)

# --- –¶–µ–ø–æ—á–∫–∞ –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ ---
def friendly_chain(user_input, chat_history):
    response = llm.invoke([
        SystemMessage(content="–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫."),
        *chat_history,
        ("human", user_input)
    ])
    return response.content

# --- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é LLM ---
def is_technical_llm(question: str) -> bool:
    classification_prompt = [
        SystemMessage(content="You are a classifier. Your job is to decide if a question is technical."),
        ("human", f"""
Question: "{question}"

Is it a technical question about Linux, X-Road, DevOps, APIs, infrastructure or system administration?

Reply only with YES or NO.
""")
    ]
    result = llm.invoke(classification_prompt)
    return "yes" in result.content.strip().lower()

# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ ---
def enhanced_query(query: str) -> dict:
    if is_technical_llm(query):
        result = acurai_chain.invoke({
            "input": query,
            "chat_history": memory.chat_memory.messages
        })
        answer_text = result["answer"].content if hasattr(result["answer"], "content") else str(result["answer"])
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(answer_text)
        return {
            "answer": answer_text
        }
    else:
        answer = friendly_chain(query, memory.chat_memory.messages)
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(answer)
        return {
            "answer": answer
        }
# --- –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–æ–ª—å ---
if __name__ == "__main__":
    while True:
        user_input = input("–í—ã: ")
        if user_input.lower() in ("exit", "quit", "–≤—ã—Ö–æ–¥"):
            break
        response = enhanced_query(user_input)
        print("GPT:", response["answer"])
import os
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

load_dotenv()

# ðŸ’¬ Ð§Ð°Ñ‚-Ð¼Ð¾Ð´ÐµÐ»ÑŒ OpenAI
llm = ChatOpenAI(model='gpt-4o', temperature=0)

# ðŸ“š ÐŸÐ¾Ð´Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð±Ð°Ð·Ñ‹
vectorstore = FAISS.load_local("faiss_index", OpenAIEmbeddings(), allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever()

# âœ… Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ AcuRAI prompt
acurai_prompt = PromptTemplate.from_template("""
You are a senior assistant for system administrators using X-Road.

Your job is to deeply analyze the user's technical problem and give a complete, precise and practical answer.

Always reason through the following steps (internally), but show only the final ANSWER:

---
QUESTION: {question}
TASK: What does the user want to achieve?
SYMPTOM: What is going wrong?
CONTEXT: Use documentation, known issues, and common misconfigurations.
---

ANSWER: 
Respond as if you were helping a DevOps engineer in production.

Always include:
- âœ… What the user is trying to do.
- ðŸ” Possible root causes (numbered list).
- ðŸ”§ Steps to investigate (with file paths, commands, log names).
- ðŸ“„ Example config snippets (if applicable).
- ðŸ“‚ Exact file locations and required permissions.
- ðŸ“˜ Mention source documentation files.

Style:
- Use markdown formatting (headers, code blocks, bullet points).
- Prefer short, structured blocks over long paragraphs.
- Avoid vague phrases â€” be concrete.

Only show the ANSWER section in your response.
""")

# ðŸ§  ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð°
memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")

# ðŸ”— Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ QA
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    combine_docs_chain_kwargs={
        "prompt": acurai_prompt
    },
    return_source_documents=True,
    verbose=True,
    output_key="answer"
)

def enhanced_query(query: str) -> dict:
    result = qa_chain.invoke({
        "question": query,
        "chat_history": memory.chat_memory.messages
    })
    return result
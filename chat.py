import os
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import create_history_aware_retriever, create_retrieval_chain

# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÑÑ€ÐµÐ´Ñ‹
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ LLM
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.2,
    api_key=OPENAI_API_KEY
)

# Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ FAISS
vectorstore = FAISS.load_local(
    folder_path="faiss_index",
    embeddings=OpenAIEmbeddings(api_key=OPENAI_API_KEY),
    allow_dangerous_deserialization=True
)

# ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ñ‡Ð°Ñ‚Ð°
memory = ConversationBufferMemory(
    return_messages=True,
    memory_key="chat_history"
)

# --- SYSTEM PROMPT Ð´Ð»Ñ AcuRAI ---
acurai_system = SystemMessage(
    content="""
You are a senior assistant for system administrators using X-Road.

Your job is to deeply analyze the user's technical problem and give a complete, precise and practical answer.

Always reason through the following steps (internally), but show only the final ANSWER.

---
ANSWER:
âœ… What the user is trying to do
ðŸ” Possible root causes (numbered list)
ðŸ”§ Steps to investigate:
- With **commands**, **paths**, **logs**
- What to look for (e.g., errors, confirmation lines)
ðŸ“„ Example config snippets (if applicable)
ðŸ“‚ Exact file locations and required permissions
ðŸ“˜ Source documentation references (with filenames or markdown anchors)
ðŸ§ª Can the issue be verified via CLI or UI?
ðŸªµ What logs to check?
âš ï¸ Are there tricky or common mistakes?

Style:
- Use markdown formatting (headers, code blocks, bullet points)
- Use emoji bullets for clarity
- Avoid vague phrases â€” be concrete

Only output the ANSWER section. Do not show your reasoning process.
"""
)

# --- PROMPT Ð´Ð»Ñ AcuRAI Ñ€ÐµÐ¶Ð¸Ð¼Ð° (Ð±ÐµÐ· context!) ---
acurai_prompt = ChatPromptTemplate.from_messages([
    acurai_system,
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])

# --- PROMPT Ð´Ð»Ñ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ð¾Ð³Ð¾ Ñ€ÐµÐ¶Ð¸Ð¼Ð° ---
friendly_prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="Ð¢Ñ‹ â€” Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¿Ñ€Ð¾ÑÑ‚Ð¾, ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾ Ð¸ Ð²ÐµÐ¶Ð»Ð¸Ð²Ð¾."),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])

# --- Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸-Ð¾ÑÐ¾Ð·Ð½Ð°Ð½Ð½Ñ‹Ð¹ retriever ---
retriever = create_history_aware_retriever(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    prompt=acurai_prompt
)

combine_chain = acurai_prompt | llm

acurai_chain = create_retrieval_chain(
    retriever=retriever,
    combine_docs_chain=combine_chain
)

# --- Ð¦ÐµÐ¿Ð¾Ñ‡ÐºÐ° Ð´Ð»Ñ Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° ---
def friendly_chain(user_input, chat_history):
    response = llm.invoke([
        SystemMessage(content="Ð¢Ñ‹ â€” Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº."),
        *chat_history,
        ("human", user_input)
    ])
    return response.content

# --- ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ LLM ---
def is_technical_llm(question: str) -> bool:
    classification_prompt = [
        SystemMessage(content="You are a classifier. Your job is to decide if a question is technical."),
        ("human", f"""
Question: "{question}"

Is it a technical question about Linux, X-Road, DevOps, APIs, infrastructure or system administration?

Reply only with YES or NO.
""")
    ]
    result = llm.invoke(classification_prompt)
    return "yes" in result.content.strip().lower()

# --- ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¾Ð³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° ---
def enhanced_query(query: str) -> dict:
    if is_technical_llm(query):
        result = acurai_chain.invoke({
            "input": query,
            "chat_history": memory.chat_memory.messages
        })
        answer_text = str(result["answer"])  # <-- Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(answer_text)
        return {
            "answer": answer_text
        }
    else:
        answer = friendly_chain(query, memory.chat_memory.messages)
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(answer)
        return {
            "answer": answer
        }
# --- Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°Ð¿ÑƒÑÐº Ñ‡ÐµÑ€ÐµÐ· ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ ---
if __name__ == "__main__":
    while True:
        user_input = input("Ð’Ñ‹: ")
        if user_input.lower() in ("exit", "quit", "Ð²Ñ‹Ñ…Ð¾Ð´"):
            break
        response = enhanced_query(user_input)
        print("GPT:", response["answer"])
import os
from dotenv import load_dotenv

from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.2,
    api_key=OPENAI_API_KEY
)

# –®–∞–±–ª–æ–Ω AcuRAI –±–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π {context}

acurai_prompt = PromptTemplate(
    input_variables=["question", "context"],
    template="""
You are a senior assistant for system administrators using X-Road.

Your job is to deeply analyze the user's technical problem and give a complete, precise and practical answer.

Always reason through the following steps (internally), but show only the final ANSWER.

---
QUESTION: {question}
TASK: What does the user want to achieve?
SYMPTOM: What is going wrong?
CONTEXT: {context}
---

ANSWER:
Respond as if you were helping a DevOps engineer in production.

Always include:

‚úÖ What the user is trying to do

üîç Possible root causes (numbered list)

üîß Steps to investigate:
- With **commands**, **paths**, **logs**
- What to look for (e.g., errors, confirmation lines)

üìÑ Example config snippets (if applicable)

üìÇ Exact file locations and required permissions

üìò Source documentation references (with filenames or markdown anchors)

---

üß™ Can the issue be verified via CLI or UI?
- Say **what to do** in command line
- Say **what to click** in UI

ü™µ What logs to check?
- Which **log files**
- What to **search for**

‚ö†Ô∏è Are there tricky or common mistakes?
- What users **frequently miss or misconfigure**

---

Style:
- Use markdown formatting (headers, code blocks, bullet points)
- Use emoji bullets for clarity
- Avoid vague phrases ‚Äî be concrete

Only show the final ANSWER section in your response.
"""
)

# –ü–∞–º—è—Ç—å —á–∞—Ç–∞
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ FAISS
vectorstore = FAISS.load_local(
    folder_path="faiss_index",
    embeddings=OpenAIEmbeddings(api_key=OPENAI_API_KEY),
    allow_dangerous_deserialization=True
)

# –¶–µ–ø–æ—á–∫–∞ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    memory=memory,
    return_source_documents=True,
    combine_docs_chain_kwargs={"prompt": acurai_prompt},
    output_key="answer",
    verbose=True
)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
def enhanced_query(query: str) -> dict:
    result = qa_chain.invoke({
        "question": query,
        "chat_history": memory.chat_memory.messages
    })
    return {
        "answer": result["answer"],
        "source_documents": [
            doc.metadata.get("source", "") for doc in result["source_documents"]
        ],
        "chat_history": memory.chat_memory.messages
    }

# –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫
if __name__ == "__main__":
    while True:
        user_input = input("–í—ã: ")
        if user_input.lower() in ("exit", "quit", "–≤—ã—Ö–æ–¥"):
            break
        response = enhanced_query(user_input)
        print("GPT:", response["answer"])
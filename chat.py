import os
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories.in_memory import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_history_aware_retriever, create_retrieval_chain

# –ó–∞–≥—Ä—É–∑–∫–∞ API-–∫–ª—é—á–∞
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.2,
    api_key=OPENAI_API_KEY
)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ FAISS-–∏–Ω–¥–µ–∫—Å–∞
vectorstore = FAISS.load_local(
    folder_path="faiss_index",
    embeddings=OpenAIEmbeddings(api_key=OPENAI_API_KEY),
    allow_dangerous_deserialization=True
)
retriever = vectorstore.as_retriever()

# üîß –®–∞–±–ª–æ–Ω —Ç–æ–ª—å–∫–æ —Å 'input', –ø–æ—Ç–æ–º—É —á—Ç–æ 'create_history_aware_retriever' —Ç—Ä–µ–±—É–µ—Ç —ç—Ç–æ!
contextualize_q_prompt = PromptTemplate(
    input_variables=["input"],
    template="Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {input}\nStandalone question:"
)

# üß† –°–æ–∑–¥–∞–Ω–∏–µ retriever —Å –∏—Å—Ç–æ—Ä–∏–µ–π
retriever_with_history = create_history_aware_retriever(
    llm=llm,
    retriever=retriever,
    prompt=contextualize_q_prompt
)

# üîß –®–∞–±–ª–æ–Ω –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
combine_prompt = PromptTemplate(
    input_variables=["context", "input"],
    template="""
You are an expert assistant for system administrators working with X-Road documentation.
Your task is to analyze technical problems, investigate causes, and give clear instructions.

QUESTION: {input}
CONTEXT: {context}
ANSWER: 
Respond with a clear, structured and helpful answer that includes:

- What the user is trying to do.
- What could be the causes of the problem.
- Steps to investigate the issue (e.g. log file paths, commands).
- Specific instructions to fix or confirm the behavior.
- Example configuration snippets (if needed).
- Mention exact filenames, directories or UI locations (if applicable).

Only show the ANSWER section in your response.
"""
)

# –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏
qa_chain = create_retrieval_chain(
    retriever=retriever_with_history,
    combine_docs_chain_kwargs={"prompt": combine_prompt}
)

# –§—É–Ω–∫—Ü–∏—è –ø–∞–º—è—Ç–∏ (–≤ RAM, –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ Redis –∏ —Ç.–¥.)
def get_memory(session_id: str) -> BaseChatMessageHistory:
    return ChatMessageHistory()

# –û–±—ë—Ä—Ç–∫–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
qa_chain_with_memory = RunnableWithMessageHistory(
    qa_chain,
    get_session_history=get_memory,
    input_messages_key="input",
    history_messages_key="chat_history"
)

# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
def enhanced_query(query: str) -> dict:
    result = qa_chain_with_memory.invoke(
        {"input": query},
        config={"configurable": {"session_id": "user"}}
    )
    return {
        "answer": result["answer"],
        "source_documents": [
            doc.metadata.get("source", "") for doc in result.get("source_documents", [])
        ]
    }

# –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
if __name__ == "__main__":
    while True:
        query = input("–í—ã: ")
        if query.lower() in ("exit", "quit", "–≤—ã—Ö–æ–¥"):
            break
        answer = enhanced_query(query)
        print("GPT:", answer["answer"])
import os
from dotenv import load_dotenv
from functools import partial
from pathlib import Path

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# –ó–∞–≥—Ä—É–∑–∫–∞ .env
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö Markdown-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
loader = DirectoryLoader(
    "data",
    glob="**/*.md",
    loader_cls=partial(TextLoader, encoding='utf-8')
)
documents = loader.load()

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤
documents = [doc for doc in documents if doc.page_content.strip()]
print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(documents)}")

# –í—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
for doc in documents[:3]:
    print(f"\nüìÅ {doc.metadata['source']}")
    print(doc.page_content[:300], "...\n")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
split_docs = text_splitter.split_documents(documents)
print(f"‚úÇÔ∏è –†–∞–∑–±–∏—Ç–æ –Ω–∞ —á–∞–Ω–∫–∏: {len(split_docs)}")

# –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
embeddings = OpenAIEmbeddings(api_key=openai_api_key)

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
vectorstore = FAISS.from_documents(split_docs, embeddings)
vectorstore.save_local("faiss_index")
print("‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ø–∞–ø–∫—É faiss_index.")